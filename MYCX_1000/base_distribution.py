import requests
import pandas as pd
import numpy as np
import time
import json
from datetime import datetime, timedelta
import os
from chinese_calendar import is_workday
from io import BytesIO
from matplotlib.figure import Figure
from matplotlib.backends.backend_agg import FigureCanvasAgg

# Module-level session to enable connection reuse and avoid FD leaks
HTTP_SESSION = requests.Session()
try:
    adapter = requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=20, max_retries=3)
    HTTP_SESSION.mount('http://', adapter)
    HTTP_SESSION.mount('https://', adapter)
except Exception:
    pass

# ================= é…ç½®åŒºåŸŸ =================
EVENT_RANGE = range(200, 300) 
BASE_URL = "https://bestdori.com/api/"
SERVER = 3 # å›½æœ
OUTPUT_FILE = "base_speed_distribution.json"

# ================= æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def fetch_event_meta(event_id):
    """è·å–æ´»åŠ¨å…ƒæ•°æ®"""
    try:
        meta_url = f"{BASE_URL}events/{event_id}.json"
        r = HTTP_SESSION.get(meta_url, timeout=5)
        r.raise_for_status()
        metadata = r.json()
        return {
            "event_id": event_id,
            "start_at": int(metadata["startAt"][SERVER]),
            "end_at": int(metadata["endAt"][SERVER]),
            "event_type": metadata.get("eventType", "unknown")
        }
    except:
        return None

def fetch_tier_1000_data(event_id):
    """è·å– T1000 åˆ†æ•°çº¿æ•°æ® (Tracker API)"""
    tracker_url = f"{BASE_URL}tracker/data?server={SERVER}&event={event_id}&tier=1000"
    try:
        r = HTTP_SESSION.get(tracker_url, timeout=10)
        r.raise_for_status()
        tracker_data = r.json()
        if not tracker_data["result"]:
            return None
        return pd.DataFrame(tracker_data["cutoffs"])
    except:
        return None

def fetch_top10_max_speed(event_id):
    """
    è·å– T10 æ•°æ®å¹¶è®¡ç®—è¯¥æ´»åŠ¨ç†è®ºæœ€å¤§é€Ÿåº¦ (Scale Factor)
    API: eventtop
    """
    # ä½¿ç”¨ 1å°æ—¶ (3600000ms) çš„é—´éš”æ¥è·å–è¾ƒä¸ºå¹³æ»‘çš„æé€Ÿï¼Œé¿å…ç¬æ—¶çˆ†å‘çš„å™ªå£°
    url = f"{BASE_URL}eventtop/data?server={SERVER}&event={event_id}&mid=0&interval=3600000"
    
    try:
        r = HTTP_SESSION.get(url, timeout=10)
        r.raise_for_status()
        data = r.json()
        if not data or "points" not in data:
            return None
        
        # è½¬æ¢ä¸º DataFrame
        # åªéœ€è¦å‰ 200 æ¡æ•°æ®é€šå¸¸è¶³å¤Ÿè¦†ç›–å¼€å±€çˆ†å‘æœŸï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œ
        # æˆ‘ä»¬è¿™é‡Œå–å‰ 500 æ¡ä»¥ç¡®ä¿è¦†ç›–åˆ°è‡³å°‘å‡ å°æ—¶çš„æ•°æ®é‡ï¼Œ
        # å› ä¸º eventtop è¿”å›çš„æ˜¯æ‰€æœ‰ top10 ç©å®¶çš„ç‚¹ï¼Œ10ä¸ªç©å®¶æ¯å°æ—¶1ä¸ªç‚¹ï¼Œ100æ¡åªå¤Ÿ10å°æ—¶ã€‚
        df = pd.DataFrame(data["points"]).head(500)
        
        if df.empty:
            return None
            
        # ä¼˜åŒ–ï¼šä»…å¤„ç†å‰ N æ¡æ•°æ®ä»¥èŠ‚çœæ€§èƒ½
        # df = df.head(500) 
        
        # === æ ¸å¿ƒé€»è¾‘ï¼šæŒ‰ UID åˆ†ç»„è®¡ç®—é€Ÿåº¦ ===
        # 1. æ’åº
        df = df.sort_values(by=["uid", "time"])
        
        # 2. è®¡ç®—å·®åˆ† (æ¯ä¸ª UID å†…éƒ¨è®¡ç®—)
        df["pt_diff"] = df.groupby("uid")["value"].diff()
        df["time_diff"] = df.groupby("uid")["time"].diff()
        
        # 3. è®¡ç®—é€Ÿåº¦ (EP / åˆ†é’Ÿ)
        # time_diff å•ä½æ˜¯æ¯«ç§’ï¼Œæ‰€ä»¥è¦ / 1000 / 60
        df["speed"] = df["pt_diff"] / (df["time_diff"] / 1000 / 60)
        
        # 4. æ¸…æ´—æ•°æ®
        # å»é™¤ NaN (ç¬¬ä¸€æ¡æ•°æ®æ²¡æœ‰å·®åˆ†)
        # å»é™¤ é€Ÿåº¦ < 0 (å¯èƒ½æ˜¯æ‰æ¡£æˆ–æ•°æ®é”™è¯¯)
        # å»é™¤ é€Ÿåº¦è¿‡å¤§
        valid_speeds = df[(df["speed"] > 0) & (df["speed"] < 1000000)]["speed"]
        
        if valid_speeds.empty:
            return None
            
        # 5. è·å–æé€Ÿ
        # å–æœ€å¤§çš„å‰å‡ ä¸ªå€¼çš„å¹³å‡ï¼Œæˆ–è€…ç›´æ¥å–æœ€å¤§å€¼ï¼ˆéœ€æ’é™¤æç«¯å¼‚å¸¸å€¼ï¼‰
        # è¿™é‡Œé‡‡ç”¨å– Top 3 çš„å¹³å‡å€¼ä½œä¸º Scale Factorï¼Œæ¯”å•ä¸€æœ€å¤§å€¼æ›´ç¨³å®š
        top_speeds = valid_speeds.nlargest(3).values
        if len(top_speeds) > 0:
            return np.mean(top_speeds)
        else:
            return valid_speeds.max()
            
    except Exception as e:
        print(f"Error fetching T10 for {event_id}: {e}")
        return None

def calculate_speed_tracker(df):
    """è®¡ç®— Tracker æ•°æ® (T1000) çš„é€Ÿåº¦"""
    df = df.sort_values("time")
    df["ep_diff"] = df["ep"].diff()
    df["time_diff"] = df["time"].diff() / 1000 / 60
    df["speed"] = df["ep_diff"] / df["time_diff"]
    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=["speed"])
    df = df[df["speed"] >= 0]
    return df

def get_day_type(dt):
    """åˆ¤æ–­æ—¥æœŸç±»å‹ (å·¥ä½œæ—¥ vs å‘¨æœ«)"""
    # 1. ä¼˜å…ˆåˆ¤æ–­ç‰¹å®šæ—¶é—´æ®µçš„æ¨¡å¼åˆ‡æ¢
    # å‘¨äº” 17:00 å -> å‘¨æœ«æ¨¡å¼
    if dt.weekday() == 4 and dt.hour >= 17:
        return "weekend"
    # å‘¨æ—¥ 23:00 å -> å·¥ä½œæ—¥æ¨¡å¼
    if dt.weekday() == 6 and dt.hour >= 23:
        return "weekday"

    # 2. ä½¿ç”¨ chinesecalendar (å¦‚æœå¯ç”¨)
    if is_workday is not None:
        try:
            if is_workday(dt.date()):
                return "weekday"
            else:
                return "weekend"
        except:
            pass

    # 3. Fallback
    if dt.weekday() >= 5:
        return "weekend"
    return "weekday"

# ================= ä¸»é€»è¾‘ =================

def main():
    distribution_data = {
        "weekday": {h: [] for h in range(24)},
        "weekend": {h: [] for h in range(24)}
    }
    
    valid_event_count = 0
    print("ğŸ± CatGPT æ­£åœ¨å¯åŠ¨åˆ†æå¼•æ“å–µ...")

    for event_id in EVENT_RANGE:
        # 1. è·å–åŸºç¡€ä¿¡æ¯
        meta = fetch_event_meta(event_id)
        if not meta:
            continue
            
        # 2. è·å– T1000 æ•°æ®
        df_1000 = fetch_tier_1000_data(event_id)
        if df_1000 is None or df_1000.empty:
            continue
            
        # 3. ã€æ–°é€»è¾‘ã€‘è·å– T10 æé€Ÿä½œä¸º Scale Factor
        scale_factor = fetch_top10_max_speed(event_id)
        
        if not scale_factor or scale_factor < 100: # é€Ÿåº¦å¤ªå°è¯´æ˜æ•°æ®æœ‰é—®é¢˜
            print(f"âš ï¸ Event {event_id}: æ— æ³•è®¡ç®—æœ‰æ•ˆçš„ T10 æé€Ÿï¼Œè·³è¿‡ã€‚")
            continue
            
        # 4. å¤„ç† T1000 æ•°æ®
        df_1000 = calculate_speed_tracker(df_1000)
        
        # å½’ä¸€åŒ–
        df_1000["norm_speed"] = df_1000["speed"] / scale_factor
        
        # 5. ç­›é€‰æœ‰æ•ˆæ—¶é—´æ®µ (æ’é™¤é¦–æ—¥24hï¼Œå°¾æ—¥48h)
        start_ts = meta["start_at"]
        end_ts = meta["end_at"]
        valid_start = start_ts + 24 * 3600 * 1000
        valid_end = end_ts - 48 * 3600 * 1000
        
        df_valid = df_1000[(df_1000["time"] >= valid_start) & (df_1000["time"] <= valid_end)].copy()
        
        # è½¬æ¢æ—¶é—´
        df_valid["dt"] = pd.to_datetime(df_valid["time"], unit="ms") + timedelta(hours=8)
        
        # å¡«å…¥æ•°æ®æ¡¶
        for _, row in df_valid.iterrows():
            hour = row["dt"].hour
            day_type = get_day_type(row["dt"])
            
            # è¿‡æ»¤å¼‚å¸¸å½’ä¸€åŒ–å€¼ (T1000 é€Ÿåº¦ä¸åº”è¶…è¿‡ T10 æé€Ÿå¤ªå¤š)
            if 0 <= row["norm_speed"] <= 1.2: 
                distribution_data[day_type][hour].append(row["norm_speed"])
        
        valid_event_count += 1
        print(f"âœ… Event {event_id} | T10æé€Ÿ: {scale_factor:.0f} EP/min | å·²å½’æ¡£")
        time.sleep(0.5)

    # ================= èšåˆè¾“å‡º =================
    final_distribution = {"weekday": {}, "weekend": {}}
    
    for dtype in ["weekday", "weekend"]:
        for h in range(24):
            speeds = distribution_data[dtype][h]
            if speeds:
                final_distribution[dtype][h] = {
                    "mean": float(np.mean(speeds)),
                    "median": float(np.median(speeds)),
                    "std": float(np.std(speeds)),
                    "count": len(speeds)
                }
            else:
                final_distribution[dtype][h] = None

    with open(OUTPUT_FILE, "w") as f:
        json.dump(final_distribution, f, indent=4)

    # ç»˜å›¾éƒ¨åˆ†ä¿æŒä¸å˜ (çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œé€»è¾‘åŒä¸Š)
    print(f"\nğŸ‰ åˆ†æç»“æŸï¼å…±å¤„ç† {valid_event_count} ä¸ªæ´»åŠ¨ã€‚")
    print(f"æ•°æ®å·²ä¿å­˜è‡³ {OUTPUT_FILE}ï¼ŒKey 0-23 ä»£è¡¨æ¯å¤©çš„å°æ—¶æ®µå–µï¼")
    # ç”Ÿæˆå¯è§†åŒ–è¾“å‡ºï¼ˆå¦‚æœ matplotlib å¯ç”¨ï¼‰
    try:
        def plot_distribution(dist, out_prefix="distribution"):
            hours = list(range(24))

            def extract_stats(dtype):
                medians = []
                means = []
                counts = []
                for h in hours:
                    val = dist[dtype].get(h)
                    if val:
                        medians.append(val["median"])
                        means.append(val["mean"])
                        counts.append(val["count"])
                    else:
                        medians.append(float('nan'))
                        means.append(float('nan'))
                        counts.append(0)
                return medians, means, counts

            for dtype in ["weekday", "weekend"]:
                medians, means, counts = extract_stats(dtype)

                fig = Figure(figsize=(10, 4))
                ax1 = fig.add_subplot(1, 1, 1)
                ax1.plot(hours, medians, marker='o', label='median')
                ax1.plot(hours, means, marker='x', label='mean', alpha=0.7)
                ax1.set_xlabel('Hour')
                ax1.set_ylabel('Normalized Speed')
                ax1.set_title(f'{dtype.capitalize()} hourly normalized-speed distribution')
                ax1.set_xticks(hours)
                ax1.grid(axis='y', linestyle='--', alpha=0.3)
                ax1.legend(loc='upper left')

                ax2 = ax1.twinx()
                ax2.bar(hours, counts, color='gray', alpha=0.2, label='count')
                ax2.set_ylabel('Count')
                ax2.set_ylim(0, max(counts) * 1.2 if max(counts) > 0 else 1)

                out_dir = os.path.dirname(OUTPUT_FILE) or '.'
                os.makedirs(out_dir, exist_ok=True)
                out_path = os.path.join(out_dir, f"{out_prefix}_{dtype}.png")
                try:
                    fig.tight_layout()
                except Exception:
                    pass
                # Prefer fig.savefig; if that fails in headless env, fallback to Agg canvas
                try:
                    fig.savefig(out_path)
                except Exception:
                    try:
                        buf = BytesIO()
                        FigureCanvasAgg(fig).print_png(buf)
                        with open(out_path, 'wb') as f:
                            f.write(buf.getvalue())
                        buf.close()
                    except Exception as e:
                        print(f"Failed to save plot {out_path}: {e}")
                finally:
                    try:
                        fig.clf()
                        del fig
                    except Exception:
                        pass
                print(f"Saved plot: {out_path}")

        plot_distribution(final_distribution, out_prefix=os.path.splitext(OUTPUT_FILE)[0])
    except Exception as e:
        print(f"Plotting failed: {e}")

if __name__ == "__main__":
    main()